{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare"
      ],
      "metadata": {
        "id": "28wo1ezEdGzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get raw data"
      ],
      "metadata": {
        "id": "FCrEUFaFdPUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1D62SLrqV2Qo3amiihH8rDpY_GcZGIL34"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhx0Z74KzyhW",
        "outputId": "e8584221-5ff9-4a66-c4e0-a135c94356be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1D62SLrqV2Qo3amiihH8rDpY_GcZGIL34\n",
            "To: /content/ub_sample_data.csv\n",
            "\r  0% 0.00/1.78M [00:00<?, ?B/s]\r100% 1.78M/1.78M [00:00<00:00, 114MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install pyspark"
      ],
      "metadata": {
        "id": "C_z1s1SidRmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSltseHI4TTl",
        "outputId": "6ed6fc0a-7794-419f-e40d-5848da018e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=3949f051da5fd212b4d0d9d1be96c6dd75971adda8dd5bc55abc3e000c070939\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "UiDLZge5dT_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu24Fb6j-KUR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import date, timedelta, datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initiate pypark"
      ],
      "metadata": {
        "id": "jx5_jspSdXP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"PysparkExample\")\\\n",
        "    .config (\"spark.sql.shuffle.partitions\", \"50\")\\\n",
        "    .config(\"spark.driver.maxResultSize\",\"5g\")\\\n",
        "    .config (\"spark.sql.execution.arrow.enabled\", \"true\")\\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "TAqpvUrD-OzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare data "
      ],
      "metadata": {
        "id": "oqjHf0m0ddcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make spark dataframe from raw data file"
      ],
      "metadata": {
        "id": "g-_7mxbPil-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df = spark.read.csv('ub_sample_data.csv', header = True )\n",
        "sample_df"
      ],
      "metadata": {
        "id": "l-3f86jf-RBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2adb20-9430-4aed-ffc1-08304f1fa9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[user_id: string, business_id: string]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8EQ6IzN2qP8",
        "outputId": "2172d699-489b-4eeb-a075-186b8acb74c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|             user_id|         business_id|\n",
            "+--------------------+--------------------+\n",
            "|39FT2Ui8KUXwmUt6h...|RJSFI7mxGnkIIKiJC...|\n",
            "|39FT2Ui8KUXwmUt6h...|fThrN4tfupIGetkrz...|\n",
            "|39FT2Ui8KUXwmUt6h...|mvLdgkwBzqllHWHwS...|\n",
            "|39FT2Ui8KUXwmUt6h...|uW6UHfONAmm8QttPk...|\n",
            "|39FT2Ui8KUXwmUt6h...|T70pMoTP008qYLsIv...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Register table to use Sql script"
      ],
      "metadata": {
        "id": "ug7TbMP-iusn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df.registerTempTable(\"sample_df\")"
      ],
      "metadata": {
        "id": "GzCQuajX3DJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae7b397-672d-4c86-9210-27a9c70fe292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
            "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL script to get edge from raw data"
      ],
      "metadata": {
        "id": "i997jooKjxOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edge_df = spark.sql('''\n",
        "  select u1, u2\n",
        "  from \n",
        "      (select  df1.user_id u1, df2.user_id u2\n",
        "      from sample_df df1, sample_df df2\n",
        "      where df1.business_id = df2.business_id \n",
        "        and df1.user_id < df2.user_id)\n",
        "  group by u1, u2\n",
        "  having count(u1) >= 7\n",
        "''')"
      ],
      "metadata": {
        "id": "xw94da2p4KFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHMEvobu6oCm",
        "outputId": "198ab428-1120-4ce7-dee6-0a8cdebb8dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "498"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export edge data to csv to use in task 2."
      ],
      "metadata": {
        "id": "bR5BWczcj6BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# edge_df.toPandas().to_csv('edge_df.csv', index=False)"
      ],
      "metadata": {
        "id": "yCraG9sJhx_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 1:"
      ],
      "metadata": {
        "id": "r3Ua170bj_bQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Function used in task 1"
      ],
      "metadata": {
        "id": "xkVfLuyhkJ2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Somehow, sum() function cannot work\n",
        "# So I need to loop through the list to get the sum.\n",
        "\n",
        "\n",
        "def get_data(graph, edge_df):\n",
        "    # create edges for the graph from edge spark dataframe\n",
        "\n",
        "    for row in edge_df.collect():\n",
        "        u1 = row['u1']\n",
        "        u2 = row['u2']\n",
        "\n",
        "        if (u1 not in graph.keys()) and (u2 not in graph.keys()):\n",
        "            # if two users are not in the graph.\n",
        "            graph[u1] = {u2: 0}\n",
        "            graph[u2] = {u1: 0}\n",
        "        elif (u1 not in graph.keys()):\n",
        "            # If user1 not in the graph\n",
        "            graph[u1] = {u2: 0}\n",
        "            graph[u2][u1] = 0\n",
        "        elif (u2 not in graph.keys()):\n",
        "            # If user2 not in the graph\n",
        "            graph[u2] = {u1: 0}\n",
        "            graph[u1][u2] = 0\n",
        "        else:\n",
        "            # If two users are already in the graph\n",
        "            graph[u1][u2] = 0\n",
        "            graph[u2][u1] = 0\n",
        "\n",
        "\n",
        "def reset_weight(graph):\n",
        "    # reset weight (betweeness) of the graph to 0\n",
        "    for node in graph.keys():\n",
        "        neighbors = graph[node].keys()\n",
        "        weights = [0]*len(neighbors)\n",
        "        graph[node] = {neighbor: weight for neighbor, weight in zip(neighbors, weights)}\n",
        "\n",
        "\n",
        "def BFS_gen_tree(graph, root):\n",
        "    # Use BFS to create a tree\n",
        "    # Each level is a dictionary\n",
        "    # Level n + 1 contain child nodes of one or more node in level n\n",
        "\n",
        "    # current_node = root\n",
        "\n",
        "    # Setup for level 0\n",
        "    visited_nodes = [root]\n",
        "    tree = [{root:{'number_shortest_path': 1, 'credit': 1}}]\n",
        "    level = 1\n",
        "\n",
        "    # Loop until cannot add more node into the tree\n",
        "    while True:\n",
        "        current_level_nodes = []\n",
        "\n",
        "        # add nodes to the current level nodes\n",
        "        for node in tree[level - 1].keys():\n",
        "            current_level_nodes += [neighbor for neighbor in graph[node] if neighbor not in visited_nodes]\n",
        "\n",
        "        # add nodes to the tree\n",
        "        if len(current_level_nodes) > 0:\n",
        "            visited_nodes += current_level_nodes\n",
        "        else:\n",
        "            # If there is no node, stop the loop\n",
        "            break\n",
        "        \n",
        "        # Create current_level\n",
        "        current_level = {}\n",
        "        for node in current_level_nodes:\n",
        "            # Find parents for the node\n",
        "            parents = [parent for parent in tree[level-1].keys() if parent in graph[node].keys()]\n",
        "\n",
        "            # get the number of shortest path of the node by loop though its parents\n",
        "            parent_points = [tree[level -1][parent]['number_shortest_path'] for parent in parents]\n",
        "\n",
        "            number_shortest_path = 0\n",
        "            for point in parent_points:\n",
        "                number_shortest_path += point\n",
        "\n",
        "            # add the node to current level with its number of shortest path and credit\n",
        "            # Credit of the node is initiate to 1 because at any point, the credit of each node will be added by 1\n",
        "            current_level[node] = {'number_shortest_path': number_shortest_path, 'credit': 1}\n",
        "\n",
        "        # append current level to the tree\n",
        "        tree.append(current_level)\n",
        "\n",
        "        # increase level\n",
        "        level += 1\n",
        "    return tree\n",
        "\n",
        "\n",
        "def calc_betweeness(graph, tree):\n",
        "    # calculate the betweeness of each edge in the graph with the BFS tree\n",
        "\n",
        "    # number of level is n\n",
        "    # level number run from n-1 to 1\n",
        "    # at each level calculate the betweeness of the edges that lead to the parent's level\n",
        "    # and calculate the credit of parent nodes\n",
        "    for level_num in range(len(tree) - 1, 0, -1):\n",
        "        # get the current level\n",
        "        level = tree[level_num]\n",
        "\n",
        "        # get the parent level\n",
        "        parent_level = tree[level_num -1]\n",
        "\n",
        "        # loop through the current level and compute for the parent level\n",
        "        for node in level:\n",
        "            # get the parents of the node\n",
        "            parents = [parent for parent in parent_level if parent in graph[node].keys()]\n",
        "\n",
        "            # calculate credit for the edges lead to the parents\n",
        "            credit = level[node]['credit'] / len(parents)\n",
        "\n",
        "            # compute credit and betweeness for the parents and edges\n",
        "            for parent in parents:\n",
        "                # add credit for the parent nodes\n",
        "                parent_level[parent]['credit'] += credit\n",
        "\n",
        "                # add betweeness for the edges\n",
        "                # devide by 2 because each edge will be compute 2 times\n",
        "                graph[parent][node] += credit/2\n",
        "                graph[node][parent] += credit/2\n",
        "    \n",
        "\n",
        "def remove_between_edge(graph):\n",
        "    # remove the edge with highest betweenness\n",
        "\n",
        "    # setup highest betweenness and node to remove\n",
        "    between_edge = (0,0)\n",
        "    high_betweeness = 0\n",
        "\n",
        "    # get nodes in the graph.\n",
        "    nodes = list(graph.keys())\n",
        "\n",
        "    # loop through each node, get edge's betweenness \n",
        "    # and update highest betweenness and node to remove\n",
        "    for node in nodes:\n",
        "        # loop through neighbors of the node\n",
        "        for neighbor in graph[node].keys():\n",
        "            # get edge's betweenness(edge: [node, neighbor])\n",
        "            betweeness = graph[node][neighbor]\n",
        "\n",
        "            # update edge to remove and highest betweenness\n",
        "            if betweeness > high_betweeness:\n",
        "                between_edge = (node, neighbor)\n",
        "                high_betweeness = betweeness\n",
        "\n",
        "    # remove the edge in the undirected graph \n",
        "    # remove two edges: [u1, u2], [u2, u1]\n",
        "    graph[between_edge[0]].pop(between_edge[1])\n",
        "    graph[between_edge[1]].pop(between_edge[0])\n",
        "\n",
        "\n",
        "def get_communities(graph ):\n",
        "    # Use BFS to get communities\n",
        "    # add all nodes to the to_visit list.\n",
        "    # start with a random node in the list to be a root\n",
        "    # BFS until there is no node to add to the community\n",
        "    # (nodes are in community is remove from to_visit list)\n",
        "    # repeat with another node in the to_visit list until there is no node to visit\n",
        "\n",
        "    # setup\n",
        "    nodes_to_visit = list(graph.keys())\n",
        "    communities = []\n",
        "\n",
        "    # stop when there is no node in to_visit list\n",
        "    while len(nodes_to_visit) > 0:\n",
        "        # set a random node to be a root of BFS\n",
        "        community = [nodes_to_visit[0]]\n",
        "\n",
        "\n",
        "\n",
        "        # i is an index of node in the community (node_i)\n",
        "        # each loop will add neighbor of node_i into the community\n",
        "        # stop when there is no new node in the community(index i out of community range)\n",
        "        i = 0\n",
        "        while i < len(community):\n",
        "            # loop through the neighbor of node_i\n",
        "            for neighbor in list(graph[community[i]].keys()):\n",
        "                # add neighbor into community if it is not already in community\n",
        "                if neighbor not in community:\n",
        "                    community.append(neighbor)\n",
        "            # increase index\n",
        "            i+=1\n",
        "\n",
        "        # add community to the community list\n",
        "        communities.append(community)\n",
        "\n",
        "        # remove node in community from to_visit list\n",
        "        nodes_to_visit = [node for node in nodes_to_visit if node not in community]\n",
        "\n",
        "    # return list of communities\n",
        "    return communities\n",
        "\n",
        "\n",
        "def modularity(graph, communities, A, m):\n",
        "    # calculate modularity with the formula in task 1\n",
        "\n",
        "    # setup\n",
        "    total_score = 0\n",
        "\n",
        "    # loop through each community in list of communities\n",
        "    for community in communities:\n",
        "        # setup modularity for the cummunity\n",
        "        score = 0\n",
        "\n",
        "        # loop through each node in the community\n",
        "        for node1 in community:\n",
        "            # get degree of node1\n",
        "            k1 = len(A[node1])\n",
        "            \n",
        "            # loop through each node in the community once more time \n",
        "            for node2 in community:\n",
        "                # get degree of node2\n",
        "                k2 = len(A[node2])  \n",
        "\n",
        "                # check if there an edge between two nodes\n",
        "                Aij = 1 if node2 in A[node1] else 0\n",
        "\n",
        "                # update modularity of the community\n",
        "                score += (Aij - (float(k1*k2)/(2*m)))\n",
        "\n",
        "        # update modularity of graph (communities)\n",
        "        total_score += score\n",
        "\n",
        "    # return normalized value of modularity\n",
        "    return total_score / (2*m)\n",
        "\n",
        "\n",
        "def modularity2(graph, communities, A, m):\n",
        "    # calculate modularity follow the shorten formula privide in networkx modularity\n",
        "    # which was used to calculate the modularity by networkx\n",
        "    # We do this to try to get the exact number of modularity that networkx produced\n",
        "\n",
        "    # setup\n",
        "    total_score = 0\n",
        "\n",
        "    # loop through each community\n",
        "    for community in communities:\n",
        "\n",
        "        # calculate inside link of the community \n",
        "        # which is a link that link two nodes inside of the community\n",
        "\n",
        "        # sum function broken so i loop through the list.\n",
        "        # inside_degree = sum([sum([1 if neighbor in community else 0 for neighbor in A[node] ]) for node in community ]) / 2\n",
        "\n",
        "        # calculate inside degree\n",
        "        inside_degree_list = [[1 if neighbor in community else 0 for neighbor in A[node] ] for node in community ]\n",
        "        inside_degree = 0\n",
        "        for row in inside_degree_list:\n",
        "            for i in row:\n",
        "                inside_degree += i\n",
        "\n",
        "        # calculate total inside link which is total inside degree divide by 2\n",
        "        inside_link = inside_degree / 2\n",
        "\n",
        "        # calculate total degree of all nodes in community\n",
        "        \n",
        "        # full_degree = sum([sum(len(graph[node].keys())) for node in community]) /2\n",
        "        full_degree_list = [len(A[node]) for node in community]\n",
        "\n",
        "        full_degree = 0\n",
        "        for i in full_degree_list:\n",
        "            full_degree += i\n",
        "\n",
        "        # calculate modularity of the community \n",
        "        # and update modulairty of the comminites\n",
        "        total_score += ((inside_link / m ) - (full_degree / (2*m ))**2 )\n",
        "\n",
        "    # return modularity\n",
        "    return total_score\n",
        "\n",
        "\n",
        "def girvan_newman(ori_graph , modularity=modularity):\n",
        "    # copy the graph to compute on just a copy\n",
        "    # the original graph will not be affect\n",
        "    graph = ori_graph.copy()\n",
        "\n",
        "    # define a calculate number of edges in a graph\n",
        "    # (graph to store data, not a graphic graph)\n",
        "    def calc_edges(graph, nodes):\n",
        "        num_edges = 0\n",
        "        edges_list = [len(list(graph[node].keys())) for node in nodes]\n",
        "        for edges in edges_list:\n",
        "            num_edges += edges\n",
        "        return num_edges\n",
        "\n",
        "    # set up\n",
        "    nodes = list(graph.keys())\n",
        "\n",
        "    # neighbors_list is a list of neighber respected to node in nodes\n",
        "    neighbors_list = [list(graph[node].keys()) for node in nodes]\n",
        "\n",
        "    # get the original adjacent list and the number of edges of the graph\n",
        "    A = {node: neighbors for node, neighbors in zip(nodes, neighbors_list)}\n",
        "    m = calc_edges(graph, nodes)/2\n",
        "    \n",
        "    # set up\n",
        "    best_modularity = -1000\n",
        "    best_communities = []\n",
        "\n",
        "\n",
        "    # loop until there is no edge in the graph\n",
        "    while calc_edges(graph, nodes) > 0:\n",
        "        # loop through the nodes\n",
        "        for node in nodes:\n",
        "            # generate tree with root is the current node\n",
        "            tree = BFS_gen_tree(graph, node)\n",
        "\n",
        "            # calculate betweenness with the tree just maked\n",
        "            calc_betweeness(graph, tree)\n",
        "\n",
        "        # remove the edge with highest betweenness\n",
        "        remove_between_edge(graph)\n",
        "\n",
        "        # reset edge's betweenness to 0\n",
        "        reset_weight(graph)\n",
        "\n",
        "        # get the communities after remove the edge\n",
        "        communities = get_communities(graph)\n",
        "\n",
        "        # calculate modularity of the graph\n",
        "        score = modularity(graph, communities, A, m)\n",
        "\n",
        "        # update the best modularity\n",
        "        # and the communities with the best modularity\n",
        "        if score > best_modularity:\n",
        "            best_modularity = score\n",
        "            best_communities = communities\n",
        "\n",
        "    # return the best communities in the formated follow the instruction and its modularities\n",
        "    return (sorted(sorted([sorted(community) for community in best_communities],key=lambda x: x[0] ), key=len), best_modularity)"
      ],
      "metadata": {
        "id": "4VnyUKXU-4it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a graph and get data for it"
      ],
      "metadata": {
        "id": "THDEPgjuMOms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = {}\n",
        "get_data(graph, edge_df)"
      ],
      "metadata": {
        "id": "-AMgbiPFCRNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 1.1"
      ],
      "metadata": {
        "id": "7DdhQKUKOC6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get nodes in graph\n",
        "nodes = list(graph.keys())\n",
        "\n",
        "# loop through each node\n",
        "for node in nodes: \n",
        "    # make BFS tree with the current node is root\n",
        "    tree = BFS_gen_tree(graph, node)\n",
        "\n",
        "    # calculate betweennes of the edges\n",
        "    calc_betweeness(graph, tree)"
      ],
      "metadata": {
        "id": "nqRI_ixKS7NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort nodes \n",
        "nodes.sort()\n",
        "\n",
        "# visited nodes store all the nodes that have visited \n",
        "# because there is 2 edge represent for 1 real each: \n",
        "# Ex: [u1,u2] is the same as [u2,u1]\n",
        "# when we get all the edges like: [u1, *]\n",
        "# add u1 to the visited nodes so that\n",
        "# those edges like: [*, u1], will not be get.\n",
        "\n",
        "visited_nodes = []\n",
        "result1 = []\n",
        "\n",
        "# loop through the nodes\n",
        "for node in nodes:\n",
        "    # if node is not visit\n",
        "    if node not in visited_nodes:\n",
        "        # get all the edges (like [node, neighbor]) with its betweenness \n",
        "        # if the neighbor is not in visited_nodes\n",
        "        temp_result1 = [[(node, neighbor), graph[node][neighbor]] for neighbor in list(graph[node].keys()) if neighbor not in visited_nodes]\n",
        "\n",
        "        # add those to the result\n",
        "        result1 += temp_result1\n",
        "\n",
        "        # add node to visited\n",
        "        visited_nodes.append(node)"
      ],
      "metadata": {
        "id": "yETFDZTbT3Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort descending by betweennes\n",
        "result1.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# write result to file\n",
        "result1 = [', '.join([str(i) for i in item]) for item in result1]\n",
        "with open(\"task1_1_result.txt\", \"w\") as file1:\n",
        "    file1.write('\\n'.join(result1))"
      ],
      "metadata": {
        "id": "f8H54Rd4ayIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reset betweeennes for next task\n",
        "reset_weight(graph)"
      ],
      "metadata": {
        "id": "ToAmsvXlfSMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 1.2"
      ],
      "metadata": {
        "id": "uaVJU__oedjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run girvan_newman and print out best_modularity\n",
        "best_communities, best_modularity = girvan_newman(graph)\n",
        "best_modularity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ5mGd7jS7Kc",
        "outputId": "175e187a-0b1f-4254-dcdc-46d19dfce84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6872550442734674"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write result to file\n",
        "result = str(best_communities).replace('], ','\\n').replace('[','').replace(']]','')\n",
        "with open(\"task1_2_result.txt\", \"w\") as file2:\n",
        "    file2.write(result)\n",
        "    file2.write('\\n'+str(best_modularity))"
      ],
      "metadata": {
        "id": "Rx7z8koUAqIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Try to replicate the modulairty of networkx module"
      ],
      "metadata": {
        "id": "tKS8XNwgxo4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# graph2 = graph.copy()\n",
        "graph2 = {}\n",
        "get_data(graph2, edge_df)\n",
        "best_communities, best_modularity = girvan_newman(graph2, modularity=modularity2)\n",
        "best_modularity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPosW_PkjZW4",
        "outputId": "d62a4789-255e-445b-c43a-460b23f09999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6872550442734798"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write result to file\n",
        "result = str(best_communities).replace('], ','\\n').replace('[','').replace(']]','')\n",
        "with open(\"task1_2_result2.txt\", \"w\") as file2:\n",
        "    file2.write(result)\n",
        "    file2.write('\\n'+str(best_modularity))"
      ],
      "metadata": {
        "id": "D5jIXbdtSECF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}